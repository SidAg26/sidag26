<!DOCTYPE html>
<html lang="en" class="dark">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Intelligent Cloud Autoscaling: Unleashing Reinforcement Learning in Serverless Environments - Siddharth Agarwal</title>
    <meta name="description" content="Serverless computing has transformed how developers build and deploy applications, offering unparalleled agility, reduced operational overhead, and a pay-per-execution cost model. However, realizing the full potential of serverless platforms like AWS Lambda, Azure Functions, or Google Cloud Functions hinges critically on efficient resource management, particularly autoscaling. While serverless providers offer built-in autoscaling, these mechanisms are often reactive, relying on predefined thresholds and historical averages, which can lead to suboptimal performance (e.g., cold starts, high latency) during traffic spikes or underutilization and increased costs during lulls.">
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            darkMode: 'class',
            theme: {
                extend: {
                    colors: {
                        primary: '#58a6ff',
                        secondary: '#c9d1d9',
                        dark: '#0d1117',
                        'dark-lighter': '#161b22'
                    }
                }
            }
        }
    </script>
    
    <style>
        /* Styling unchanged from your template */
        .gradient-bg { background: linear-gradient(135deg, #0d1117 0%, #161b22 100%); }
        .blog-content { line-height: 1.8; font-size: 1.1rem; }
        .blog-content h2 { color: #58a6ff; font-size: 1.8rem; font-weight: 700; margin-top: 2rem; margin-bottom: 1rem; }
        .blog-content h3 { color: #ffffff; font-size: 1.4rem; font-weight: 600; margin-top: 1.5rem; margin-bottom: 0.75rem; }
        .blog-content p { margin-bottom: 1.5rem; color: #c9d1d9; }
        .blog-content ul, .blog-content ol { margin-bottom: 1.5rem; padding-left: 1.5rem; }
        .blog-content li { margin-bottom: 0.5rem; color: #c9d1d9; }
        .blog-content code { background: rgba(88, 166, 255, 0.1); color: #58a6ff; padding: 0.2rem 0.4rem; border-radius: 0.25rem; font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace; }
        .blog-content pre { background: #1a1a1a; border: 1px solid #333; border-radius: 0.5rem; padding: 1rem; overflow-x: auto; margin: 1.5rem 0; }
        .blog-content pre code { background: none; color: #e6e6e6; padding: 0; }
        .tag { display: inline-block; padding: 0.25rem 0.5rem; margin: 0.125rem; border-radius: 0.375rem; font-size: 0.75rem; font-weight: 500; background: rgba(88, 166, 255, 0.1); color: #58a6ff; border: 1px solid rgba(88, 166, 255, 0.3); }
        .back-button { background: rgba(88, 166, 255, 0.1); color: #58a6ff; border: 1px solid rgba(88, 166, 255, 0.3); padding: 0.5rem 1rem; border-radius: 0.5rem; transition: all 0.3s ease; }
        .back-button:hover { background: rgba(88, 166, 255, 0.2); transform: translateY(-2px); }
        .table-of-contents { background: rgba(255, 255, 255, 0.05); border: 1px solid rgba(255, 255, 255, 0.1); border-radius: 0.75rem; padding: 1.5rem; margin: 2rem 0; }
        .table-of-contents ul { list-style: none; padding: 0; }
        .table-of-contents li { margin-bottom: 0.5rem; }
        .table-of-contents a { color: #58a6ff; text-decoration: none; transition: color 0.2s ease; }
        .table-of-contents a:hover { color: #ffffff; }
    </style>
</head>
<body class="bg-dark text-secondary min-h-screen">
    <!-- Header -->
    <header class="gradient-bg border-b border-gray-800 sticky top-0 z-50 backdrop-blur-lg">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex items-center justify-between h-16">
                <h1 class="text-2xl font-bold text-primary">üöÄ Siddharth Agarwal</h1>
                <nav class="hidden md:flex space-x-8">
                    <a href="../index.html#about" class="text-secondary hover:text-primary transition-colors">About</a>
                    <a href="../index.html#research" class="text-secondary hover:text-primary transition-colors">Research</a>
                    <a href="../index.html#publications" class="text-secondary hover:text-primary transition-colors">Publications</a>
                    <a href="../index.html#experience" class="text-secondary hover:text-primary transition-colors">Experience</a>
                    <a href="../index.html#contact" class="text-secondary hover:text-primary transition-colors">Contact</a>
                </nav>
                <a href="../index.html" class="back-button">‚Üê Back to Portfolio</a>
            </div>
        </div>
    </header>

    <!-- Article Header -->
    <section class="gradient-bg py-20">
        <div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="text-center mb-8">
                <a href="./index.html" class="text-primary hover:underline mb-4 inline-block">‚Üê Back to Blog</a>
            </div>
            <h1 class="text-4xl md:text-5xl font-bold text-white mb-6">üöÄ Intelligent Cloud Autoscaling: Unleashing Reinforcement Learning in Serverless Environments</h1>
            <p class="text-xl text-secondary mb-8">Serverless computing has transformed how developers build and deploy applications, offering unparalleled agility, reduced operational overhead, and a pay-per-execution cost model. However, realizing the full potential of serverless platforms like AWS Lambda, Azure Functions, or Google Cloud Functions hinges critically on efficient resource management, particularly autoscaling. While serverless providers offer built-in autoscaling, these mechanisms are often reactive, relying on predefined thresholds and historical averages, which can lead to suboptimal performance (e.g., cold starts, high latency) during traffic spikes or underutilization and increased costs during lulls.</p>
            <div class="flex flex-wrap justify-center gap-2 mb-6">
                <span class="tag">Serverless</span> <span class="tag">Cloud Computing</span> <span class="tag">Scaling</span> <span class="tag">Auto-scaling</span>
            </div>
            <div class="flex items-center justify-center text-gray-400 text-sm space-x-4">
                <span>üìÖ August 24, 2025</span>
                <span>üìñ 13</span>
                <span>üî¨ Research</span>
            </div>
        </div>
    </section>

    <!-- Article Content -->
    <article class="py-20 bg-dark">
        <div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8">
            <!-- Table of Contents -->
            <div class="table-of-contents">
                <h3 class="text-lg font-bold text-white mb-4">üìã Table of Contents</h3>
                <ul><li><a href="#introduction">Introduction</a></li><li><a href="#core-concepts-and-fundamentals">Core Concepts and Fundamentals</a></li><li><a href="#implementation-strategies-and-best-practices">Implementation Strategies and Best Practices</a></li><li><a href="#advanced-techniques-and-optimization">Advanced Techniques and Optimization</a></li><li><a href="#real-world-applications-and-case-studies">Real-World Applications and Case Studies</a></li><li><a href="#conclusion-and-future-considerations">Conclusion and Future Considerations</a></li></ul>
            </div>

            <!-- Article Body -->
            <div class="blog-content">
                <h2 id="introduction">Introduction</h2>
<p>Serverless computing has transformed how developers build and deploy applications, offering unparalleled agility, reduced operational overhead, and a pay-per-execution cost model. However, realizing the full potential of serverless platforms like AWS Lambda, Azure Functions, or Google Cloud Functions hinges critically on efficient resource management, particularly autoscaling. While serverless providers offer built-in autoscaling, these mechanisms are often reactive, relying on predefined thresholds and historical averages, which can lead to suboptimal performance (e.g., cold starts, high latency) during traffic spikes or underutilization and increased costs during lulls.</p>
<p>The dynamic and often unpredictable nature of serverless workloads presents a significant challenge for traditional autoscaling. Cold starts, where a function takes longer to execute due to initial environment setup, can severely impact user experience. Conversely, over-provisioning to avoid cold starts leads to unnecessary expenditure. The ideal autoscaler needs to be proactive, intelligent, and capable of learning from complex, evolving traffic patterns to make optimal scaling decisions that balance performance, cost, and availability.</p>
<p>This is where Reinforcement Learning (RL) emerges as a game-changer. RL, a branch of artificial intelligence, allows an 'agent' to learn optimal behaviors by interacting with an 'environment' and receiving 'rewards' or 'penalties' for its actions. By framing autoscaling as an RL problem, we can train an intelligent agent to observe real-time metrics, predict future demand, and dynamically adjust concurrency limits or provisioned instances, moving beyond rigid rules to achieve truly adaptive and efficient resource allocation in serverless environments. This blog post will explore the exciting intersection of RL and serverless autoscaling, detailing its core concepts, implementation, and future potential.</p>
<pre><code>// Traditional reactive autoscaling logic (simplified)
const scaleBasedOnCPU = (currentCPU, threshold, currentInstances, maxInstances) => {
  if (currentCPU > threshold && currentInstances < maxInstances) {
    return currentInstances + 1; // Scale up
  } else if (currentCPU < threshold * 0.5 && currentInstances > 1) {
    return currentInstances - 1; // Scale down
  }
  return currentInstances;
};

// Monitoring performance metrics
const getFunctionMetrics = async (functionName) => {
  const response = await cloudProviderSDK.getMetrics({
    FunctionName: functionName,
    MetricNames: ['Invocations', 'Errors', 'Duration', 'ConcurrentExecutions'],
    Period: 60 // seconds
  });
  return response.MetricData;
};</code></pre>
<h2 id="core-concepts-and-fundamentals">Core Concepts and Fundamentals</h2>
<p>At its heart, Reinforcement Learning for serverless autoscaling involves an agent learning to make a sequence of decisions to maximize a cumulative reward. Let's break down the fundamental components: the Agent, Environment, State, Action, and Reward. The **Agent** is our intelligent autoscaler, responsible for observing the serverless platform and making scaling decisions. The **Environment** encompasses the serverless function, its underlying infrastructure, and the incoming workload. This includes the cloud provider's API for managing concurrency, monitoring tools, and the actual traffic patterns.</p>
<p>The **State** represents a snapshot of the environment at any given time. For serverless autoscaling, a state could include current metrics like the number of active invocations, queue length, average latency, CPU utilization, memory usage, cold start rates, and even historical workload patterns over the last few minutes or hours. A well-defined state is crucial as it provides the agent with all the necessary information to make an informed decision. For instance, knowing the current invocation rate and the trend of past rates allows the agent to anticipate future demand.</p>
<p>An **Action** is a decision made by the agent to modify the environment. In serverless autoscaling, actions typically involve adjusting the concurrency limit for a function, provisioning additional instances, or de-provisioning idle resources. The granularity of these actions can vary ‚Äì from increasing concurrency by a fixed number to scaling by a percentage, or even setting specific instance counts. The agent's goal is to choose actions that transition the environment into states that yield higher rewards.</p>
<p>The **Reward** function is perhaps the most critical component, as it defines the objective the agent is trying to optimize. For serverless autoscaling, the reward function must balance conflicting goals: minimizing cost (by reducing idle resources and cold starts) and maximizing performance (by ensuring low latency and high throughput). A positive reward might be given for successfully handling a traffic surge with minimal latency and no cold starts, while a negative reward (penalty) could be assigned for excessive cold starts, high latency, or over-provisioning. Designing an effective reward function requires careful consideration of business priorities and operational constraints.</p>
<pre><code>// Example of a simplified State representation for an RL agent
class ServerlessAutoscalingState {
  constructor(invocationRate, queueLength, avgLatency, coldStartRate, currentConcurrency) {
    this.invocationRate = invocationRate; // e.g., requests per second
    this.queueLength = queueLength;     // Number of pending requests
    this.avgLatency = avgLatency;       // Average execution time
    this.coldStartRate = coldStartRate; // Percentage of invocations experiencing cold starts
    this.currentConcurrency = currentConcurrency; // Current provisioned concurrency
  }

  toVector() {
    return [this.invocationRate, this.queueLength, this.avgLatency, this.coldStartRate, this.currentConcurrency];
  }
}

// Example of defining possible Actions for the RL agent
const ScalingActions = {
  SCALE_UP_SMALL: 5,   // Increase concurrency by 5
  SCALE_UP_MEDIUM: 10, // Increase concurrency by 10
  SCALE_DOWN_SMALL: -5, // Decrease concurrency by 5
  SCALE_DOWN_MEDIUM: -10, // Decrease concurrency by 10
  NO_CHANGE: 0
};

const applyAction = async (functionName, currentConcurrency, actionDelta) => {
  const newConcurrency = Math.max(1, currentConcurrency + actionDelta);
  // Call cloud provider SDK to update concurrency
  await cloudProviderSDK.updateFunctionConcurrency(functionName, newConcurrency);
  return newConcurrency;
};</code></pre>
<h2 id="implementation-strategies-and-best-practices">Implementation Strategies and Best Practices</h2>
<p>Implementing an RL-driven autoscaler for serverless requires a systematic approach, starting with robust data collection. The agent needs comprehensive and real-time metrics to accurately perceive the environment's state. This includes not only standard cloud metrics like invocation counts, errors, and duration but also custom metrics such as queue sizes, end-to-end latency, and crucially, cold start occurrences and durations. Aggregating and normalizing this data is essential for creating a consistent state representation for the RL agent. Leveraging cloud provider monitoring services (e.g., CloudWatch, Azure Monitor) and custom instrumentation within functions can provide the necessary telemetry.</p>
<p>A critical best practice is the use of a realistic **simulation environment** for training the RL agent. Directly training an agent in a production environment is risky due to the potential for erratic scaling decisions leading to outages or excessive costs. A well-designed simulator can mimic the serverless platform's behavior, including cold start dynamics, concurrency limits, and the impact of scaling actions on latency and throughput, under various workload patterns. This allows the agent to explore different policies and learn optimal strategies in a safe, controlled manner before deployment.</p>
<p>The **reward function design** is paramount. It must accurately reflect the desired balance between cost and performance. A common approach is to formulate a composite reward that penalizes high latency, cold starts, and idle resources, while rewarding efficient resource utilization and successful request processing. For example, `Reward = - (Latency_Penalty + ColdStart_Penalty + Cost_Penalty)`. Fine-tuning the weights of these penalties is an iterative process that depends on the specific application's requirements. An overly aggressive cost penalty might lead to performance degradation, while an overly lenient one could result in higher bills.</p>
<p>Defining the **action space** and **state space** thoughtfully is also key. The action space should offer sufficient granularity for effective scaling without being overly complex, which could hinder learning. For example, instead of scaling by single instances, actions could be to increase/decrease concurrency by 10% or by fixed increments. The state space should capture all relevant information without introducing unnecessary noise or redundancy. Techniques like feature engineering and state aggregation can help simplify complex environments. Finally, **observability** of the RL agent's decisions and their impact is crucial post-deployment. Monitoring the agent's chosen actions, the resulting system metrics, and the calculated rewards allows for continuous evaluation and refinement of the RL model and its reward function.</p>
<pre><code>// Example of a composite reward function for serverless autoscaling
const calculateReward = (metrics, costPerInstanceHour, targetLatencyMs, maxColdStartRate) => {
  const { avgLatencyMs, coldStartRate, activeInstances, idleInstances, invocations } = metrics;
  let reward = 0;

  // Performance penalty: Higher latency -> lower reward
  const latencyPenalty = Math.max(0, (avgLatencyMs - targetLatencyMs) / targetLatencyMs) * 100; // Scale penalty
  reward -= latencyPenalty;

  // Cold start penalty: High cold start rate -> lower reward
  const coldStartPenalty = Math.max(0, (coldStartRate - maxColdStartRate) / maxColdStartRate) * 200; // Scale penalty
  reward -= coldStartPenalty;

  // Cost penalty: Idle instances -> lower reward
  const costPenalty = idleInstances * costPerInstanceHour; // Simple cost model
  reward -= costPenalty;

  // Throughput reward: Reward for handling invocations efficiently
  reward += invocations * 0.1; // Small positive reward for work done

  return reward;
};

// Example of state aggregation (simplifying continuous metrics into bins)
const aggregateState = (rawMetrics) => {
  const { invocationRate, queueLength, avgLatencyMs, coldStartRate, currentConcurrency } = rawMetrics;
  return {
    invocationRateBucket: Math.floor(invocationRate / 100), // e.g., 0-99, 100-199
    queueLengthBucket: Math.floor(queueLength / 50),
    latencyBucket: Math.floor(avgLatencyMs / 50),
    coldStartBucket: Math.floor(coldStartRate / 0.1),
    concurrencyBucket: Math.floor(currentConcurrency / 20)
  };
};</code></pre>
<h2 id="advanced-techniques-and-optimization">Advanced Techniques and Optimization</h2>
<p>As serverless architectures grow in complexity, advanced RL techniques become essential for optimal autoscaling. **Deep Reinforcement Learning (DRL)**, which combines RL with deep neural networks, is particularly powerful for handling high-dimensional state and action spaces. Instead of manually defining features for the state, a DRL agent can learn relevant features directly from raw telemetry data, enabling it to discern subtle patterns in workload behavior that might be missed by simpler models. This allows for more nuanced and accurate scaling decisions, especially in environments with highly variable and non-linear traffic patterns.</p>
<p>For microservices architectures, where multiple serverless functions interact and depend on each other, **Multi-Agent Reinforcement Learning (MARL)** offers a promising avenue. In MARL, each function or service could have its own RL agent, or a coordinating agent could manage a group of services. These agents learn to collaborate or compete to achieve global system objectives, such as minimizing overall cost while maintaining end-to-end latency across a complex transaction flow. This approach can prevent local optimizations from leading to global sub-optimality.</p>
<p>**Transfer Learning** can significantly accelerate the training process and improve the robustness of RL agents. An agent trained on synthetic data or a general workload pattern can be fine-tuned with real-world data from a specific serverless application. This reduces the need for extensive data collection and exploration in a live environment. Furthermore, integrating **predictive scaling** capabilities using time-series forecasting models (e.g., ARIMA, Prophet, or deep learning models like LSTMs) can provide the RL agent with a 'look-ahead' capability. The agent can then use these predictions as part of its state to make more proactive scaling decisions, anticipating demand spikes before they fully materialize, thereby mitigating cold starts and improving responsiveness.</p>
<p>Addressing **non-stationary workloads** and the **exploration-exploitation dilemma** are ongoing challenges. Non-stationary workloads, where traffic patterns change over time (e.g., seasonal trends, flash sales), require agents that can adapt their policies. Techniques like online learning, periodic retraining, or using adaptive learning rates can help. The exploration-exploitation dilemma involves balancing the need to explore new scaling strategies to find better ones versus exploiting known good strategies. Advanced algorithms like Upper Confidence Bound (UCB) or Thompson Sampling can help manage this trade-off effectively.</p>
<pre><code>// Example of a simple predictive scaling component (conceptual)
class WorkloadPredictor {
  constructor() {
    this.history = [];
    // In a real scenario, this would be a trained ML model (e.g., LSTM, ARIMA)
  }

  addObservation(timestamp, invocationCount) {
    this.history.push({ timestamp, invocationCount });
    // Keep history windowed, e.g., last 24 hours
    if (this.history.length > 1440) { // Assuming 1-minute intervals
      this.history.shift();
    }
  }

  predictNextHour() {
    if (this.history.length < 60) return null; // Need enough data
    // Simple moving average for demonstration; replace with ML model
    const lastHourInvocations = this.history.slice(-60).map(d => d.invocationCount);
    const average = lastHourInvocations.reduce((a, b) => a + b, 0) / lastHourInvocations.length;
    return average * 1.2; // Predict 20% increase for next hour based on average
  }
}

// Example of an adaptive threshold for scaling based on predicted load
const getAdaptiveThreshold = (predictedLoad, baseThreshold) => {
  // Adjust threshold proactively based on predicted load
  if (predictedLoad > 1000) {
    return baseThreshold * 0.8; // Be more aggressive if high load predicted
  } else if (predictedLoad < 100) {
    return baseThreshold * 1.2; // Be less aggressive if low load predicted
  }
  return baseThreshold;
};</code></pre>
<h2 id="real-world-applications-and-case-studies">Real-World Applications and Case Studies</h2>
<p>While still an evolving field, the application of Reinforcement Learning for serverless autoscaling holds immense promise across various industries. Consider an e-commerce platform experiencing highly volatile traffic due to flash sales or seasonal events. A traditional autoscaler might struggle to keep up, leading to slow page loads and abandoned carts during peak times, or over-provisioning during off-peak, incurring unnecessary costs. An RL agent, continuously learning from traffic patterns and system responses, could proactively scale resources, ensuring a smooth customer experience while optimizing infrastructure spend.</p>
<p>Another compelling use case is in data processing pipelines, particularly those involving real-time analytics or IoT data ingestion. These workloads often exhibit bursty patterns, requiring rapid scaling to prevent backlogs and ensure timely processing. An RL-driven autoscaler could dynamically adjust the concurrency of serverless functions responsible for data ingestion and transformation, maintaining low latency for critical data streams without wasting resources during periods of low activity. Early adopters and research initiatives have shown promising results, demonstrating significant improvements in cost efficiency (up to 30-40% reduction in some cases) and performance metrics (e.g., 20% reduction in average latency) compared to static or rule-based scaling.</p>
<p>However, real-world implementation comes with its own set of challenges. Data sparsity, especially for new functions or infrequent events, can hinder the agent's learning process. The exploration-exploitation dilemma needs careful management to ensure the agent explores new, potentially better scaling policies without jeopardizing production stability. Safety constraints are paramount; the RL agent must operate within predefined guardrails to prevent catastrophic over-scaling or under-scaling. Lessons learned from practical implementations emphasize the importance of robust monitoring, A/B testing, and a gradual rollout strategy, often starting with a 'shadow' mode where the RL agent makes decisions but doesn't execute them, allowing for validation against actual system behavior.</p>
<h2 id="conclusion-and-future-considerations">Conclusion and Future Considerations</h2>
<p>The integration of Reinforcement Learning into cloud autoscaling for serverless environments represents a significant leap forward in resource management. By moving beyond static rules and reactive thresholds, RL agents can learn to make proactive, intelligent scaling decisions that dynamically adapt to complex and unpredictable workloads. This not only leads to substantial cost savings through optimized resource utilization but also dramatically improves application performance by minimizing latency and mitigating cold starts, ultimately enhancing the end-user experience. The ability of RL to balance conflicting objectives of cost and performance makes it an ideal candidate for the nuanced demands of modern serverless architectures.</p>
<p>Looking ahead, the future of RL in cloud autoscaling is bright and ripe with innovation. We can anticipate more sophisticated DRL models capable of handling even higher-dimensional state spaces and more complex inter-service dependencies in microservices. The development of standardized RL platforms and toolkits specifically tailored for cloud resource management will lower the barrier to entry for developers and operations teams. Furthermore, the convergence of RL with other AI techniques, such as predictive analytics for workload forecasting and anomaly detection for identifying unusual traffic patterns, will create even more robust and autonomous cloud management systems.</p>
<p>For organizations venturing into this space, the journey involves careful planning, starting with robust data collection, building effective simulation environments, and iteratively refining reward functions. While challenges like data sparsity and the exploration-exploitation dilemma persist, the potential benefits in terms of operational efficiency and competitive advantage are immense. Embracing Reinforcement Learning for serverless autoscaling is not just an optimization; it's a strategic move towards truly autonomous, self-optimizing cloud infrastructure, paving the way for the next generation of intelligent cloud computing.</p>
            </div>

            <!-- Author Bio -->
            <div class="mt-16 p-6 bg-gray-900 rounded-xl border border-gray-800">
                <h3 class="text-lg font-bold text-white mb-4">üë®‚Äçüíª About the Author</h3>
                <p class="text-gray-300 mb-4">
                    <strong>Siddharth Agarwal</strong> is a PhD Researcher in Cloud Computing & Distributed Systems at the University of Melbourne. His research focuses on serverless computing optimization, cold start reduction, and intelligent autoscaling using reinforcement learning.
                </p>
                <div class="flex flex-wrap gap-2">
                    <a href="../index.html#research" class="tag hover:bg-primary/20 transition-colors">Research</a>
                    <a href="../index.html#publications" class="tag hover:bg-primary/20 transition-colors">Publications</a>
                    <a href="../index.html#contact" class="tag hover:bg-primary/20 transition-colors">Contact</a>
                </div>
            </div>

            <!-- Navigation -->
            <div class="mt-16 flex justify-between items-center">
                <a href="./index.html" class="text-primary hover:underline">‚Üê Back to Blog</a>
                <span class="text-gray-500">More articles coming soon!</span>
            </div>
        </div>
    </article>

    <!-- Footer -->
    <footer class="bg-gray-900 border-t border-gray-800 py-8">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
            <p class="text-gray-400">¬© 2025 Siddharth Agarwal. Built with ‚ù§Ô∏è and modern web technologies.</p>
            <div class="mt-4">
                <a href="../index.html" class="text-primary hover:underline">‚Üê Back to Portfolio</a>
            </div>
        </div>
    </footer>

    <!-- Smooth Scrolling -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            document.querySelectorAll('a[href^="#"]').forEach(anchor => {
                anchor.addEventListener('click', function (e) {
                    e.preventDefault();
                    const target = document.querySelector(this.getAttribute('href'));
                    if (target) {
                        target.scrollIntoView({ behavior: 'smooth', block: 'start' });
                    }
                });
            });
        });
    </script>
</body>
</html>
